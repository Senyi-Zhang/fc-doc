{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project Veracity: Advanced Fact-Checking System 1. Overview Project Veracity is a sophisticated fact-checking system designed to meticulously analyze textual content and identify factual inaccuracies with an exceptional degree of precision. In an era informacija (and misinformation) overload, the ability to swiftly and reliably verify the veracity of claims presented in written material is paramount. This system aims to provide a robust solution by systematically deconstructing a given text, evaluating its constituent claims against credible evidence, and delivering a clear assessment of its factual accuracy. Our primary objective is to achieve ultimate accuracy in fact-verification. 2. Problem Statement The proliferation of misinformation and disinformation poses significant challenges across various domains, from journalism and public discourse to academic research and everyday information consumption. Manually verifying long-form content is time-consuming, resource-intensive, and often impractical at scale. Existing automated solutions may lack the nuanced understanding required to achieve high accuracy, especially when dealing with complex statements, subtle inaccuracies, or claims requiring sophisticated reasoning. There is a critical need for an advanced system that can dissect text, identify verifiable claims, and rigorously assess their truthfulness with near-human levels of accuracy. 3. Proposed Solution: System Architecture Project Veracity employs an iterative, modular approach to fact-checking: Input: A long passage of text. Core Workflow: Claim Extraction: The system intelligently identifies individual, verifiable factual statements (claims) within the input text. A running processed_claims_set is maintained to avoid redundant analysis of identical claims. Evidence Retrieval: For each extracted claim, the system queries a comprehensive set of trusted knowledge sources (e.g., search engines, academic databases, knowledge graphs, reputable news archives) to gather relevant evidence. Advanced query formulation techniques are employed to maximize the relevance and quality of retrieved evidence. Evidence-Claim Relevance Analysis: The retrieved evidence is critically assessed to determine its direct relevance to the claim in question. Sophisticated semantic analysis ensures that the evidence genuinely pertains to the core assertion of the claim. Claim Verification & Factual Adjudication: If relevant evidence is found, the system evaluates the claim against this evidence to determine its factual correctness. This module assesses whether the evidence supports, refutes, or provides insufficient information regarding the claim. Accuracy Focus: To achieve ultimate accuracy, this stage (and others like claim extraction and relevance analysis) will leverage state-of-the-art Large Language Model (LLM) APIs (e.g., GPT-series, Gemini) for their advanced reasoning, comprehension, and NLI (Natural Language Inference) capabilities. Iterative Processing & Output Generation: If a claim is determined to be incorrect, the system can be configured to either: Immediately report the first identified inaccuracy. Continue processing to identify all factual errors within the passage. Successfully verified or irrelevant claims are added to the processed_claims_set . The loop continues until all unique, verifiable claims are processed or a critical error is found (based on configuration). The final output will detail any identified factual errors, along with supporting evidence and explanations. 4. Key Features & Modules Automated Claim Identification: Pinpoints discrete, fact-checkable statements from complex narratives. Multi-Source Evidence Aggregation: Gathers information from diverse and credible sources. Semantic Relevance Engine: Accurately matches evidence to claims based on meaning, not just keywords. LLM-Powered Verification Core: Utilizes cutting-edge LLM APIs for superior accuracy in claim understanding, evidence interpretation, and truthfulness assessment. Iterative Deep-Dive Analysis: Systematically processes text to ensure comprehensive coverage. Detailed Error Reporting: Provides clear explanations and cites evidence for any identified inaccuracies. Configurable Sensitivity: Allows for adjustments in how strictly claims are interpreted and how errors are reported. 5. Technology Stack (Preliminary) Core Logic: Python Natural Language Processing & Understanding: Large Language Model APIs (e.g., OpenAI GPT-series, Google Gemini) for core tasks: Advanced Claim Extraction Nuanced Evidence-Claim Relevance Analysis High-Accuracy Claim Verification & Reasoning Traditional NLP Libraries (e.g., spaCy, NLTK) for preprocessing and supplementary tasks. Evidence Retrieval: APIs for search engines (e.g., Google Search API), academic databases, and knowledge graphs. Data Management: Suitable database for storing processed claims, evidence snippets, and knowledge bases. 6. Target Audience & Use Cases 7. Expected Impact & Goals","title":"Outline"},{"location":"#project-veracity-advanced-fact-checking-system","text":"","title":"Project Veracity: Advanced Fact-Checking System"},{"location":"#1-overview","text":"Project Veracity is a sophisticated fact-checking system designed to meticulously analyze textual content and identify factual inaccuracies with an exceptional degree of precision. In an era informacija (and misinformation) overload, the ability to swiftly and reliably verify the veracity of claims presented in written material is paramount. This system aims to provide a robust solution by systematically deconstructing a given text, evaluating its constituent claims against credible evidence, and delivering a clear assessment of its factual accuracy. Our primary objective is to achieve ultimate accuracy in fact-verification.","title":"1. Overview"},{"location":"#2-problem-statement","text":"The proliferation of misinformation and disinformation poses significant challenges across various domains, from journalism and public discourse to academic research and everyday information consumption. Manually verifying long-form content is time-consuming, resource-intensive, and often impractical at scale. Existing automated solutions may lack the nuanced understanding required to achieve high accuracy, especially when dealing with complex statements, subtle inaccuracies, or claims requiring sophisticated reasoning. There is a critical need for an advanced system that can dissect text, identify verifiable claims, and rigorously assess their truthfulness with near-human levels of accuracy.","title":"2. Problem Statement"},{"location":"#3-proposed-solution-system-architecture","text":"Project Veracity employs an iterative, modular approach to fact-checking: Input: A long passage of text. Core Workflow: Claim Extraction: The system intelligently identifies individual, verifiable factual statements (claims) within the input text. A running processed_claims_set is maintained to avoid redundant analysis of identical claims. Evidence Retrieval: For each extracted claim, the system queries a comprehensive set of trusted knowledge sources (e.g., search engines, academic databases, knowledge graphs, reputable news archives) to gather relevant evidence. Advanced query formulation techniques are employed to maximize the relevance and quality of retrieved evidence. Evidence-Claim Relevance Analysis: The retrieved evidence is critically assessed to determine its direct relevance to the claim in question. Sophisticated semantic analysis ensures that the evidence genuinely pertains to the core assertion of the claim. Claim Verification & Factual Adjudication: If relevant evidence is found, the system evaluates the claim against this evidence to determine its factual correctness. This module assesses whether the evidence supports, refutes, or provides insufficient information regarding the claim. Accuracy Focus: To achieve ultimate accuracy, this stage (and others like claim extraction and relevance analysis) will leverage state-of-the-art Large Language Model (LLM) APIs (e.g., GPT-series, Gemini) for their advanced reasoning, comprehension, and NLI (Natural Language Inference) capabilities. Iterative Processing & Output Generation: If a claim is determined to be incorrect, the system can be configured to either: Immediately report the first identified inaccuracy. Continue processing to identify all factual errors within the passage. Successfully verified or irrelevant claims are added to the processed_claims_set . The loop continues until all unique, verifiable claims are processed or a critical error is found (based on configuration). The final output will detail any identified factual errors, along with supporting evidence and explanations.","title":"3. Proposed Solution: System Architecture"},{"location":"#4-key-features-modules","text":"Automated Claim Identification: Pinpoints discrete, fact-checkable statements from complex narratives. Multi-Source Evidence Aggregation: Gathers information from diverse and credible sources. Semantic Relevance Engine: Accurately matches evidence to claims based on meaning, not just keywords. LLM-Powered Verification Core: Utilizes cutting-edge LLM APIs for superior accuracy in claim understanding, evidence interpretation, and truthfulness assessment. Iterative Deep-Dive Analysis: Systematically processes text to ensure comprehensive coverage. Detailed Error Reporting: Provides clear explanations and cites evidence for any identified inaccuracies. Configurable Sensitivity: Allows for adjustments in how strictly claims are interpreted and how errors are reported.","title":"4. Key Features &amp; Modules"},{"location":"#5-technology-stack-preliminary","text":"Core Logic: Python Natural Language Processing & Understanding: Large Language Model APIs (e.g., OpenAI GPT-series, Google Gemini) for core tasks: Advanced Claim Extraction Nuanced Evidence-Claim Relevance Analysis High-Accuracy Claim Verification & Reasoning Traditional NLP Libraries (e.g., spaCy, NLTK) for preprocessing and supplementary tasks. Evidence Retrieval: APIs for search engines (e.g., Google Search API), academic databases, and knowledge graphs. Data Management: Suitable database for storing processed claims, evidence snippets, and knowledge bases.","title":"5. Technology Stack (Preliminary)"},{"location":"#6-target-audience-use-cases","text":"","title":"6. Target Audience &amp; Use Cases"},{"location":"#7-expected-impact-goals","text":"","title":"7. Expected Impact &amp; Goals"},{"location":"core_loop_code/","text":"FUNCTION FactCheckSystem_ProcessParagraph(text_paragraph): // Stores the error candidate that has the highest confidence of being incorrect found so far. // The final selection of the \"most fatal\" error and the definitive stopping strategy are deferred. most_confident_error_candidate = NULL // Example structure for most_confident_error_candidate: // { // claim_text: \"The capital of France is Berlin.\", // status: \"INCORRECT\", // confidence_of_incorrectness: 0.95, // supporting_evidence: [ {source: \"...\", snippet: \"...\"} ], // verification_details: \"Evidence clearly states Paris is the capital.\" // } // A list to store all claims that have been identified as factually incorrect, along with their details. // This could be used later if the strategy involves selecting from multiple found errors. all_identified_errors = [] // A set to keep track of claims that have already been extracted and processed to avoid redundancy. processed_claims_log = new Set() // --- MODULE 1: Claim Extraction --- // This module is responsible for extracting individual, verifiable claims from the text_paragraph. // It should ideally yield unique claims one by one. claim_extractor = InitializeClaimExtractor(text_paragraph) WHILE claim_extractor.hasMoreClaims(): current_claim_text = claim_extractor.getNextClaim() IF current_claim_text IS IN processed_claims_log: CONTINUE // Skip if already processed END IF processed_claims_log.add(current_claim_text) current_claim_processed_successfully_by_all_modules = TRUE // Flag // --- MODULE 2: Evidence Retrieval --- // For the current_claim_text, find supporting or refuting evidence. // Returns a list of evidence items (e.g., documents, snippets, URLs). retrieved_evidence_list = Module_RetrieveEvidence(current_claim_text) IF retrieved_evidence_list IS EMPTY_OR_NULL: // Cannot proceed for this claim without evidence. // Log this claim as \"Evidence Not Found\" or similar if needed. CONTINUE END IF // --- MODULE 3: Evidence-Claim Relevance Analysis --- // From retrieved_evidence_list, determine which pieces are actually relevant to current_claim_text. // Returns a collection of relevant_evidence_items and a boolean indicating if any were found. (is_any_evidence_relevant, relevant_evidence_collection) = Module_AssessEvidenceRelevance(current_claim_text, retrieved_evidence_list) IF is_any_evidence_relevant IS FALSE: // No relevant evidence found to verify this claim. // Log this claim as \"No Relevant Evidence Found\" or similar if needed. CONTINUE END IF // --- MODULE 4: Claim Verification & Confidence Scoring --- // Based on relevant_evidence_collection, verify the current_claim_text. // This module will output: // - verification_status: \"CORRECT\", \"INCORRECT\", \"UNVERIFIABLE\" / \"NEEDS_MORE_INFO\" // - confidence_score: A numeric value (e.g., 0.0 to 1.0) indicating the system's certainty in the verification_status. // - verification_details: Explanation, links to specific evidence used for judgment. (verification_status, confidence_score, verification_details_for_claim) = Module_VerifyClaimAndScoreConfidence(current_claim_text, relevant_evidence_collection) IF verification_status IS \"INCORRECT\": // An error has been identified. current_error_details = { claim_text: current_claim_text, status: \"INCORRECT\", confidence_of_incorrectness: confidence_score, supporting_evidence: relevant_evidence_collection, // Or specific evidence from verification_details verification_details: verification_details_for_claim } all_identified_errors.add(current_error_details) // Update the most_confident_error_candidate if this error is more confidently incorrect. IF most_confident_error_candidate IS NULL OR confidence_score > most_confident_error_candidate.confidence_of_incorrectness: most_confident_error_candidate = current_error_details END IF ELSE IF verification_status IS \"CORRECT\": // The claim is verified as correct. // Log or handle as per system requirements (e.g., for overall passage assessment). PRINT \"Claim: '\" + current_claim_text + \"' verified as CORRECT with confidence: \" + confidence_score ELSE IF verification_status IS \"UNVERIFIABLE\": // The claim could not be definitively verified as correct or incorrect with the available evidence. PRINT \"Claim: '\" + current_claim_text + \"' is UNVERIFIABLE with confidence: \" + confidence_score END IF // The undefined stopping strategy would be implemented around here or after the loop. // For now, the loop continues as long as claims are available. END WHILE // End of processing claims from claim_extractor // --- Output Generation (Simplified) --- // The exact output depends on the (deferred) stopping strategy and reporting requirements. // For now, if any confident error was found, we can indicate it. // Otherwise, indicate that no definitive errors were found based on the processing. IF most_confident_error_candidate IS NOT NULL: // This is the error with the highest confidence of incorrectness found during the run. // Further logic (deferred) would decide if this is the one to report, // or if another from 'all_identified_errors' is more \"fatal\" based on other criteria. RETURN most_confident_error_candidate ELSE IF all_identified_errors IS NOT EMPTY: // This case might occur if errors were found but none met a certain threshold // to become the 'most_confident_error_candidate' under specific logic not yet defined. // Or, if we just return the list for later processing. RETURN \"Multiple errors identified. Highest confidence error details: [Details of the one with highest confidence from all_identified_errors]\" // Or return all_identified_errors ELSE: RETURN \"No definitive factual errors identified in the processed claims.\" END IF END FUNCTION // --- Placeholder definitions for the Modules (to be implemented) --- FUNCTION InitializeClaimExtractor(text_paragraph): // INPUT: Raw text paragraph. // ACTION: Parses the text, identifies potential factual claims. // Manages an internal state to yield claims one by one via hasMoreClaims() and getNextClaim(). // Should ideally handle basic claim normalization and near-duplicate detection. // OUTPUT: An iterator or generator object for claims. END FUNCTION FUNCTION Module_RetrieveEvidence(claim_text_string): // INPUT: A single string representing the claim. // ACTION: Constructs search queries from the claim. // Interacts with external knowledge sources (search engine APIs, databases, etc.). // OUTPUT: A list of evidence items (e.g., URLs, text snippets, document IDs). END FUNCTION FUNCTION Module_AssessEvidenceRelevance(claim_text_string, list_of_raw_evidence): // INPUT: The claim string and the list of raw evidence items. // ACTION: For each evidence item, determines if it is semantically relevant to the claim. // May use NLP techniques like semantic similarity, NLI pre-screening. // OUTPUT: A tuple: (boolean_is_any_relevant, collection_of_relevant_evidence_items). END FUNCTION FUNCTION Module_VerifyClaimAndScoreConfidence(claim_text_string, collection_of_relevant_evidence): // INPUT: The claim string and a collection of evidence confirmed to be relevant. // ACTION: Performs the core fact-checking logic. // Compares the claim against the relevant evidence. // Determines if the claim is supported, refuted, or if evidence is insufficient. // Crucially, calculates a confidence score for this determination. // This is a key place where LLMs (like GPT or Gemini) would be heavily utilized for their // reasoning, NLI, and summarization capabilities to interpret evidence against the claim. // OUTPUT: A tuple: (string_verification_status [\"CORRECT\", \"INCORRECT\", \"UNVERIFIABLE\"], // float_confidence_score, // object_or_string_verification_details). END FUNCTION","title":"Core Loop Code"},{"location":"m2/","text":"Module 2: Evidence Retrieval (Wikipedia API Focused) 1. Goal and Approach The primary function of Module 2 is to retrieve relevant textual evidence for atomic claims (provided by Module 1) by querying the live Wikipedia database. This approach leverages Wikipedia's extensive, continuously updated, and semi-structured knowledge base without requiring a local data store. The core strategy relies on interacting with the official MediaWiki API for the relevant language edition of Wikipedia (e.g., en.wikipedia.org ). Retrieval is enhanced by Large Language Models (LLMs) which assist in query formulation, entity linking, and processing the retrieved content to extract precise evidence snippets. 2. Core Strategy: Tiered Retrieval Attempts Evidence retrieval operates iteratively. If initial attempts fail to yield relevant evidence (as determined by Module 3: Evidence Relevance Analysis), the system escalates through a series of increasingly sophisticated attempts. State is maintained for each claim, tracking the attempt_count , tried_wikipedia_titles , and tried_search_terms to ensure diversity and avoid redundant searches. Attempt 1: Direct Entity Matching & Focused Search Claim Analysis & Query Generation (LLM-assisted): The LLM analyzes the input claim_text to identify key entities and concepts. It predicts the most likely corresponding Wikipedia article title(s) for the main entities. Generates 1-2 initial queries: Title Query: Directly attempts to fetch the introduction ( prop=extracts&exintro=true&explaintext=true ) of the highest-probability article title identified. Keyword Search Query: Uses core entities and keywords in a list=search API call to find the top M (e.g., 3-5) relevant articles. These initial targets (titles/search terms) are recorded. Wikipedia API Interaction: Execute the generated queries via the MediaWiki API. For titles returned by list=search , fetch their introductory extracts. Initial Snippet Extraction: The retrieved introductory text often serves as a good initial snippet. Optionally, an LLM refines this by extracting the 1-3 sentences most directly pertinent to the input claim_text from the retrieved extract. Output: A list of potential evidence snippets [{wikipedia_article_title, relevant_snippet, wikipedia_url}, ...] is passed to Module 3. Attempt 2: Broader Search & Section Exploration (if Attempt 1 fails) Query/Strategy Refinement (LLM-assisted): The LLM receives the claim_text , the failed queries/titles from Attempt 1, and optionally feedback from Module 3. LLM Task: Generate new, broader, or alternative keyword search terms for list=search . If a generally relevant article was identified in Attempt 1 but the introduction was insufficient, the LLM suggests specific section titles within that article (e.g., \"History,\" \"Controversies,\" \"Technical specifications\") likely to contain the needed information. New search terms are recorded. Wikipedia API Interaction: Execute new keyword searches. If exploring sections: Use the API to fetch the full article text ( prop=extracts&explaintext=true ) or specific sections (e.g., via action=parse&prop=text&section=... followed by HTML-to-text conversion, or by parsing the full text). Enhanced Snippet Extraction (LLM-assisted): Since larger text blocks (full articles or sections) might be retrieved, the LLM plays a crucial role in scanning this text and extracting only the highly relevant sentences pertaining to the claim_text . Output: Evidence snippets passed to Module 3. Attempt 3: Leveraging Wikipedia Structure (if Attempts 1 & 2 fail) Strategy Adjustment (LLM-assisted): If a partially relevant article was identified but lacked direct evidence, the LLM analyzes its structure. LLM Task: Based on the claim_text and the partially relevant article's title, identify the most promising \"Categories\" the article belongs to, or relevant article titles listed in its \"See also\" section. Wikipedia API Interaction: Retrieve categories for the partially relevant article ( prop=categories ). Retrieve \"See also\" links (often requires parsing the article content). Based on LLM recommendations, perform new searches within promising categories or directly retrieve content for suggested \"See also\" articles. Snippet Extraction: As per previous attempts, using LLM assistance. Output: Evidence snippets passed to Module 3. 3. Handling Wikipedia Specifics The system incorporates logic to handle common Wikipedia features: Disambiguation Pages: If an API query returns a disambiguation page, the options are presented (potentially to an LLM) to select the most relevant target entity based on the original claim's context, triggering a follow-up query for the correct title. Redirects: The MediaWiki API typically handles redirects transparently when fetching page content by title. Content Volume: Priority is given to introductory sections and specific relevant sections identified by the LLM to manage potentially large article sizes and LLM context window limitations. 4. Role of Large Language Models (LLMs) Throughout the retrieval process, LLMs act as an \"intelligent assistant\" to: * Understand Claims: Parse the input claim to identify key semantic components. * Link Entities: Map entities in the claim to potential Wikipedia article titles. * Generate Queries: Create effective search terms and API query parameters. * Refine Strategy: Suggest alternative search angles, broader terms, or relevant sections when initial attempts fail. * Extract Snippets: Pinpoint and extract the most relevant sentences from potentially lengthy Wikipedia text. 5. API Usage Considerations Language: API calls are directed to the appropriate language version of Wikipedia (e.g., en.wikipedia.org ). Rate Limiting & Identification: Adhere to MediaWiki API usage policies, including rate limits and providing a descriptive User-Agent string in requests. Content Dynamism: Recognize that Wikipedia content can change; retrieved evidence reflects the state of the article at the time of the query. Logging timestamps and source URLs is essential. 6. Failure Condition If all tiered retrieval attempts up to the defined MAX_ATTEMPTS (e.g., 3 or 4) fail to produce evidence deemed relevant by Module 3, Module 2 reports a failure status for the claim (e.g., \"EVIDENCE_RETRIEVAL_FAILED_WIKIPEDIA\").","title":"Module2"},{"location":"m2/#module-2-evidence-retrieval-wikipedia-api-focused","text":"","title":"Module 2: Evidence Retrieval (Wikipedia API Focused)"},{"location":"m2/#1-goal-and-approach","text":"The primary function of Module 2 is to retrieve relevant textual evidence for atomic claims (provided by Module 1) by querying the live Wikipedia database. This approach leverages Wikipedia's extensive, continuously updated, and semi-structured knowledge base without requiring a local data store. The core strategy relies on interacting with the official MediaWiki API for the relevant language edition of Wikipedia (e.g., en.wikipedia.org ). Retrieval is enhanced by Large Language Models (LLMs) which assist in query formulation, entity linking, and processing the retrieved content to extract precise evidence snippets.","title":"1. Goal and Approach"},{"location":"m2/#2-core-strategy-tiered-retrieval-attempts","text":"Evidence retrieval operates iteratively. If initial attempts fail to yield relevant evidence (as determined by Module 3: Evidence Relevance Analysis), the system escalates through a series of increasingly sophisticated attempts. State is maintained for each claim, tracking the attempt_count , tried_wikipedia_titles , and tried_search_terms to ensure diversity and avoid redundant searches. Attempt 1: Direct Entity Matching & Focused Search Claim Analysis & Query Generation (LLM-assisted): The LLM analyzes the input claim_text to identify key entities and concepts. It predicts the most likely corresponding Wikipedia article title(s) for the main entities. Generates 1-2 initial queries: Title Query: Directly attempts to fetch the introduction ( prop=extracts&exintro=true&explaintext=true ) of the highest-probability article title identified. Keyword Search Query: Uses core entities and keywords in a list=search API call to find the top M (e.g., 3-5) relevant articles. These initial targets (titles/search terms) are recorded. Wikipedia API Interaction: Execute the generated queries via the MediaWiki API. For titles returned by list=search , fetch their introductory extracts. Initial Snippet Extraction: The retrieved introductory text often serves as a good initial snippet. Optionally, an LLM refines this by extracting the 1-3 sentences most directly pertinent to the input claim_text from the retrieved extract. Output: A list of potential evidence snippets [{wikipedia_article_title, relevant_snippet, wikipedia_url}, ...] is passed to Module 3. Attempt 2: Broader Search & Section Exploration (if Attempt 1 fails) Query/Strategy Refinement (LLM-assisted): The LLM receives the claim_text , the failed queries/titles from Attempt 1, and optionally feedback from Module 3. LLM Task: Generate new, broader, or alternative keyword search terms for list=search . If a generally relevant article was identified in Attempt 1 but the introduction was insufficient, the LLM suggests specific section titles within that article (e.g., \"History,\" \"Controversies,\" \"Technical specifications\") likely to contain the needed information. New search terms are recorded. Wikipedia API Interaction: Execute new keyword searches. If exploring sections: Use the API to fetch the full article text ( prop=extracts&explaintext=true ) or specific sections (e.g., via action=parse&prop=text&section=... followed by HTML-to-text conversion, or by parsing the full text). Enhanced Snippet Extraction (LLM-assisted): Since larger text blocks (full articles or sections) might be retrieved, the LLM plays a crucial role in scanning this text and extracting only the highly relevant sentences pertaining to the claim_text . Output: Evidence snippets passed to Module 3. Attempt 3: Leveraging Wikipedia Structure (if Attempts 1 & 2 fail) Strategy Adjustment (LLM-assisted): If a partially relevant article was identified but lacked direct evidence, the LLM analyzes its structure. LLM Task: Based on the claim_text and the partially relevant article's title, identify the most promising \"Categories\" the article belongs to, or relevant article titles listed in its \"See also\" section. Wikipedia API Interaction: Retrieve categories for the partially relevant article ( prop=categories ). Retrieve \"See also\" links (often requires parsing the article content). Based on LLM recommendations, perform new searches within promising categories or directly retrieve content for suggested \"See also\" articles. Snippet Extraction: As per previous attempts, using LLM assistance. Output: Evidence snippets passed to Module 3.","title":"2. Core Strategy: Tiered Retrieval Attempts"},{"location":"m2/#3-handling-wikipedia-specifics","text":"The system incorporates logic to handle common Wikipedia features: Disambiguation Pages: If an API query returns a disambiguation page, the options are presented (potentially to an LLM) to select the most relevant target entity based on the original claim's context, triggering a follow-up query for the correct title. Redirects: The MediaWiki API typically handles redirects transparently when fetching page content by title. Content Volume: Priority is given to introductory sections and specific relevant sections identified by the LLM to manage potentially large article sizes and LLM context window limitations.","title":"3. Handling Wikipedia Specifics"},{"location":"m2/#4-role-of-large-language-models-llms","text":"Throughout the retrieval process, LLMs act as an \"intelligent assistant\" to: * Understand Claims: Parse the input claim to identify key semantic components. * Link Entities: Map entities in the claim to potential Wikipedia article titles. * Generate Queries: Create effective search terms and API query parameters. * Refine Strategy: Suggest alternative search angles, broader terms, or relevant sections when initial attempts fail. * Extract Snippets: Pinpoint and extract the most relevant sentences from potentially lengthy Wikipedia text.","title":"4. Role of Large Language Models (LLMs)"},{"location":"m2/#5-api-usage-considerations","text":"Language: API calls are directed to the appropriate language version of Wikipedia (e.g., en.wikipedia.org ). Rate Limiting & Identification: Adhere to MediaWiki API usage policies, including rate limits and providing a descriptive User-Agent string in requests. Content Dynamism: Recognize that Wikipedia content can change; retrieved evidence reflects the state of the article at the time of the query. Logging timestamps and source URLs is essential.","title":"5. API Usage Considerations"},{"location":"m2/#6-failure-condition","text":"If all tiered retrieval attempts up to the defined MAX_ATTEMPTS (e.g., 3 or 4) fail to produce evidence deemed relevant by Module 3, Module 2 reports a failure status for the claim (e.g., \"EVIDENCE_RETRIEVAL_FAILED_WIKIPEDIA\").","title":"6. Failure Condition"},{"location":"m3/","text":"Module 3: Evidence-Claim Relevance Analysis 1. Goal and Importance The primary goal of Module 3 is to critically assess whether the evidence snippets retrieved by Module 2 are semantically relevant to the specific factual assertion made in the input claim_text . This is a crucial filtering step to ensure that only pertinent information proceeds to Module 4 for the actual verification process. Passing irrelevant evidence downstream would lead to incorrect verification outcomes and wasted computational resources. 2. Core Technology: Leveraging Large Language Models (LLMs) This module heavily relies on the advanced natural language understanding (NLU) and natural language inference (NLI) capabilities of LLMs. As anticipated, determining semantic relevance between two pieces of text (the claim and the potential evidence) is a task where LLMs demonstrate strong performance. They can: Understand the core meaning and intent behind both the claim and the evidence. Go beyond simple keyword matching to identify contextual and semantic relationships. Recognize nuances, synonymy, and implied meanings that are critical for accurate relevance assessment. 3. Process Overview Input: The original claim_text (from Module 1). A retrieved_evidence_snippet (including its source, e.g., URL/article title, provided by Module 2). This process is typically performed for each snippet retrieved in a batch from Module 2. LLM Task & Prompting Strategy: The LLM is provided with both the claim_text and the evidence_snippet . A carefully crafted prompt instructs the LLM to determine if the evidence_snippet directly pertains to, and could potentially help verify or refute, the factual assertion made in the claim_text . The LLM is typically asked to output: A clear relevance judgment (e.g., \"Relevant,\" \"Not Relevant\"). A \"Partially Relevant\" or \"Uncertain\" category might also be used, potentially with different downstream handling. (Recommended) A brief justification for its decision, particularly if the evidence is deemed \"Not Relevant.\" This aids in system debugging, performance analysis, and potential refinement of Module 2's retrieval strategies. Output: For each evidence snippet: A relevance status (e.g., is_relevant: true/false ). The module will pass forward only those evidence snippets identified as \"Relevant.\" (Optional) The LLM's rationale for its judgments. 4. Key Considerations While LLMs significantly simplify this task, certain aspects require attention: Defining \"Relevance\" for Fact-Checking: The prompt must clearly define what constitutes actionable relevance. For instance, the evidence should not just be about the same general topic or entities but must address the specific factual core of the claim. Handling Nuance and Ambiguity: While LLMs are generally robust, prompts may need iterative tuning to handle subtle cases of irrelevance or highly nuanced claims effectively. Confidence Thresholds (Optional): If the LLM provides a confidence score for its relevance assessment, a system-defined threshold can be used to make the final relevance decision. Feedback Loop to Module 2: If none of the evidence snippets in a batch from Module 2 are deemed relevant, Module 3 signals this outcome. This feedback is critical for triggering the re-retrieval mechanisms in Module 2, prompting it to try different queries or sources. 5. Expected Outcome The successful execution of Module 3 results in a curated list of evidence snippets that are highly pertinent to the claim under investigation. This filtered set is then passed to Module 4, providing a solid and focused foundation for the final claim verification step","title":"Module3"},{"location":"m3/#module-3-evidence-claim-relevance-analysis","text":"","title":"Module 3: Evidence-Claim Relevance Analysis"},{"location":"m3/#1-goal-and-importance","text":"The primary goal of Module 3 is to critically assess whether the evidence snippets retrieved by Module 2 are semantically relevant to the specific factual assertion made in the input claim_text . This is a crucial filtering step to ensure that only pertinent information proceeds to Module 4 for the actual verification process. Passing irrelevant evidence downstream would lead to incorrect verification outcomes and wasted computational resources.","title":"1. Goal and Importance"},{"location":"m3/#2-core-technology-leveraging-large-language-models-llms","text":"This module heavily relies on the advanced natural language understanding (NLU) and natural language inference (NLI) capabilities of LLMs. As anticipated, determining semantic relevance between two pieces of text (the claim and the potential evidence) is a task where LLMs demonstrate strong performance. They can: Understand the core meaning and intent behind both the claim and the evidence. Go beyond simple keyword matching to identify contextual and semantic relationships. Recognize nuances, synonymy, and implied meanings that are critical for accurate relevance assessment.","title":"2. Core Technology: Leveraging Large Language Models (LLMs)"},{"location":"m3/#3-process-overview","text":"Input: The original claim_text (from Module 1). A retrieved_evidence_snippet (including its source, e.g., URL/article title, provided by Module 2). This process is typically performed for each snippet retrieved in a batch from Module 2. LLM Task & Prompting Strategy: The LLM is provided with both the claim_text and the evidence_snippet . A carefully crafted prompt instructs the LLM to determine if the evidence_snippet directly pertains to, and could potentially help verify or refute, the factual assertion made in the claim_text . The LLM is typically asked to output: A clear relevance judgment (e.g., \"Relevant,\" \"Not Relevant\"). A \"Partially Relevant\" or \"Uncertain\" category might also be used, potentially with different downstream handling. (Recommended) A brief justification for its decision, particularly if the evidence is deemed \"Not Relevant.\" This aids in system debugging, performance analysis, and potential refinement of Module 2's retrieval strategies. Output: For each evidence snippet: A relevance status (e.g., is_relevant: true/false ). The module will pass forward only those evidence snippets identified as \"Relevant.\" (Optional) The LLM's rationale for its judgments.","title":"3. Process Overview"},{"location":"m3/#4-key-considerations","text":"While LLMs significantly simplify this task, certain aspects require attention: Defining \"Relevance\" for Fact-Checking: The prompt must clearly define what constitutes actionable relevance. For instance, the evidence should not just be about the same general topic or entities but must address the specific factual core of the claim. Handling Nuance and Ambiguity: While LLMs are generally robust, prompts may need iterative tuning to handle subtle cases of irrelevance or highly nuanced claims effectively. Confidence Thresholds (Optional): If the LLM provides a confidence score for its relevance assessment, a system-defined threshold can be used to make the final relevance decision. Feedback Loop to Module 2: If none of the evidence snippets in a batch from Module 2 are deemed relevant, Module 3 signals this outcome. This feedback is critical for triggering the re-retrieval mechanisms in Module 2, prompting it to try different queries or sources.","title":"4. Key Considerations"},{"location":"m3/#5-expected-outcome","text":"The successful execution of Module 3 results in a curated list of evidence snippets that are highly pertinent to the claim under investigation. This filtered set is then passed to Module 4, providing a solid and focused foundation for the final claim verification step","title":"5. Expected Outcome"},{"location":"m4/","text":"Module 4: Claim Verification, Explanation & Confidence Scoring 1. Goal and Core Functionality Module 4 is the final analytical stage where the veracity of the input claim_text is determined based on the curated, relevant evidence_snippets provided by Module 3. Beyond a simple true/false judgment, this module aims to: Verify the Claim: Determine if the claim is supported, refuted, or if the evidence remains insufficient for a definitive conclusion. It may also identify claims that are partially true or nuanced. Generate Explanations: Produce a clear, human-readable explanation for the verification decision, referencing specific pieces of evidence. Assign a Confidence Score: Provide an estimated level of confidence in the verification status. 2. Leveraging LLMs for Verification and Explanation LLMs are central to this module's operation. Their strengths are particularly well-suited for: Natural Language Inference (NLI): LLMs can effectively determine if the evidence entails, contradicts, or is neutral towards the claim. Evidence Synthesis: They can process and synthesize information from multiple evidence snippets to form a holistic judgment. Reasoning: Modern LLMs can perform multi-step reasoning (especially when guided by techniques like chain-of-thought prompting) to connect evidence to the claim's assertions. Explanation Generation: A key strength of LLMs is their ability to articulate the \"why\" behind their conclusions, citing evidence in a coherent narrative. This is vital for system transparency and user trust. The process typically involves prompting the LLM with the claim and the set of relevant evidence snippets. The LLM is then tasked to output its assessment of the claim's truthfulness and a supporting explanation. 3. The Challenge: Assigning a Reliable Confidence Score While LLMs can express their conclusions with strong linguistic certainty, translating this into a robust, numerical confidence score is non-trivial. This is because: Lack of Inherent Probabilistic Output for Facts: Standard LLM text generations don't natively include a statistically calibrated probability of the assertion being true. Token probabilities are related to word sequence likelihood, not factual accuracy confidence. Calibration Issues: If an LLM is prompted to provide a numerical score (e.g., \"rate your confidence 0-100%\"), this score may not be well-calibrated. An LLM might consistently report high confidence even when its error rate for that level is significant. Sensitivity to Prompting and Phrasing: The expressed confidence can vary based on how the LLM is prompted. Potential for Overconfidence (\"Hallucination\"): LLMs can occasionally generate incorrect statements (hallucinations) with a high degree of apparent confidence in their language. 4. Approaches to Confidence Score Generation/Estimation Given the challenges, a multi-faceted or proxy-based approach to confidence scoring is often necessary. Here are strategies being considered: Direct Prompting for Qualitative Score & Rationale: Method: Instruct the LLM to not only verify the claim but also to provide a qualitative confidence level (e.g., \"High,\" \"Medium,\" \"Low\") or a score on a simple scale (e.g., 1-5). Crucially, the LLM must also explain why it assigns this level of confidence, citing factors like evidence quality, consistency, or ambiguity. Utility: The textual rationale for the confidence level is often more insightful than the score itself and can be presented to the user. Analysis of LLM's Explanation for Uncertainty Cues: Method: The generated explanation can be analyzed (potentially by another, smaller NLP model or keyword spotting) for linguistic markers of certainty or uncertainty (e.g., \"definitely proves\" vs. \"it seems likely,\" \"evidence suggests but does not confirm\"). Utility: Provides a textual basis for inferring confidence. Evidence-Based Heuristics (System-Derived Score): Method: The system can compute a score based on characteristics of the evidence used by the LLM for its judgment: Quantity & Consistency: Number of distinct evidence pieces supporting the conclusion. High agreement among pieces boosts confidence. Source Credibility (if available from Module 2/3): Evidence from more authoritative sources could contribute to higher confidence. Presence of Contradictions: If the LLM acknowledges and resolves contradictions, or if unresolved contradictions remain, this affects confidence. Resolved contradictions might still lead to high confidence in the final verdict; unresolved ones would lower it. Evidence Specificity: How directly and unambiguously does the evidence address the claim's core assertions? Utility: Offers a more objective, albeit indirect, measure of confidence based on the input to the LLM's reasoning process. Consistency Checks: Method: Multiple Prompts/Re-runs: Query the LLM multiple times with slight variations in the prompt or by asking it to \"re-evaluate its previous assessment.\" Consistent outcomes suggest higher stability. Self-Critique: Ask the LLM to critique its own initial verification and explanation, identifying any potential weaknesses or ambiguities. A strong, unconvincing self-critique could imply higher confidence in the original assertion. Utility: Can reveal the robustness of the LLM's conclusion. Current Thinking: Our primary focus will be on achieving accurate verification and high-quality explanations. For confidence scoring, we will likely start by combining Direct Prompting for Qualitative Score & Rationale (1) with Evidence-Based Heuristics (3). The numerical score will be presented as an estimate , with the textual explanation and the evidence characteristics providing richer context for the system's confidence. 5. Output of Module 4 The final output for a given claim will include: verification_status : (e.g., SUPPORTED, REFUTED, INSUFFICIENT_EVIDENCE, NUANCED). explanation_text : Detailed reasoning with references to evidence. confidence_score : The estimated confidence (e.g., a numerical score or qualitative label like \"High\"). confidence_rationale : Factors influencing the confidence score. cited_evidence_references : Links or identifiers to the specific evidence snippets crucial for the decision.","title":"Module4"},{"location":"m4/#module-4-claim-verification-explanation-confidence-scoring","text":"","title":"Module 4: Claim Verification, Explanation &amp; Confidence Scoring"},{"location":"m4/#1-goal-and-core-functionality","text":"Module 4 is the final analytical stage where the veracity of the input claim_text is determined based on the curated, relevant evidence_snippets provided by Module 3. Beyond a simple true/false judgment, this module aims to: Verify the Claim: Determine if the claim is supported, refuted, or if the evidence remains insufficient for a definitive conclusion. It may also identify claims that are partially true or nuanced. Generate Explanations: Produce a clear, human-readable explanation for the verification decision, referencing specific pieces of evidence. Assign a Confidence Score: Provide an estimated level of confidence in the verification status.","title":"1. Goal and Core Functionality"},{"location":"m4/#2-leveraging-llms-for-verification-and-explanation","text":"LLMs are central to this module's operation. Their strengths are particularly well-suited for: Natural Language Inference (NLI): LLMs can effectively determine if the evidence entails, contradicts, or is neutral towards the claim. Evidence Synthesis: They can process and synthesize information from multiple evidence snippets to form a holistic judgment. Reasoning: Modern LLMs can perform multi-step reasoning (especially when guided by techniques like chain-of-thought prompting) to connect evidence to the claim's assertions. Explanation Generation: A key strength of LLMs is their ability to articulate the \"why\" behind their conclusions, citing evidence in a coherent narrative. This is vital for system transparency and user trust. The process typically involves prompting the LLM with the claim and the set of relevant evidence snippets. The LLM is then tasked to output its assessment of the claim's truthfulness and a supporting explanation.","title":"2. Leveraging LLMs for Verification and Explanation"},{"location":"m4/#3-the-challenge-assigning-a-reliable-confidence-score","text":"While LLMs can express their conclusions with strong linguistic certainty, translating this into a robust, numerical confidence score is non-trivial. This is because: Lack of Inherent Probabilistic Output for Facts: Standard LLM text generations don't natively include a statistically calibrated probability of the assertion being true. Token probabilities are related to word sequence likelihood, not factual accuracy confidence. Calibration Issues: If an LLM is prompted to provide a numerical score (e.g., \"rate your confidence 0-100%\"), this score may not be well-calibrated. An LLM might consistently report high confidence even when its error rate for that level is significant. Sensitivity to Prompting and Phrasing: The expressed confidence can vary based on how the LLM is prompted. Potential for Overconfidence (\"Hallucination\"): LLMs can occasionally generate incorrect statements (hallucinations) with a high degree of apparent confidence in their language.","title":"3. The Challenge: Assigning a Reliable Confidence Score"},{"location":"m4/#4-approaches-to-confidence-score-generationestimation","text":"Given the challenges, a multi-faceted or proxy-based approach to confidence scoring is often necessary. Here are strategies being considered: Direct Prompting for Qualitative Score & Rationale: Method: Instruct the LLM to not only verify the claim but also to provide a qualitative confidence level (e.g., \"High,\" \"Medium,\" \"Low\") or a score on a simple scale (e.g., 1-5). Crucially, the LLM must also explain why it assigns this level of confidence, citing factors like evidence quality, consistency, or ambiguity. Utility: The textual rationale for the confidence level is often more insightful than the score itself and can be presented to the user. Analysis of LLM's Explanation for Uncertainty Cues: Method: The generated explanation can be analyzed (potentially by another, smaller NLP model or keyword spotting) for linguistic markers of certainty or uncertainty (e.g., \"definitely proves\" vs. \"it seems likely,\" \"evidence suggests but does not confirm\"). Utility: Provides a textual basis for inferring confidence. Evidence-Based Heuristics (System-Derived Score): Method: The system can compute a score based on characteristics of the evidence used by the LLM for its judgment: Quantity & Consistency: Number of distinct evidence pieces supporting the conclusion. High agreement among pieces boosts confidence. Source Credibility (if available from Module 2/3): Evidence from more authoritative sources could contribute to higher confidence. Presence of Contradictions: If the LLM acknowledges and resolves contradictions, or if unresolved contradictions remain, this affects confidence. Resolved contradictions might still lead to high confidence in the final verdict; unresolved ones would lower it. Evidence Specificity: How directly and unambiguously does the evidence address the claim's core assertions? Utility: Offers a more objective, albeit indirect, measure of confidence based on the input to the LLM's reasoning process. Consistency Checks: Method: Multiple Prompts/Re-runs: Query the LLM multiple times with slight variations in the prompt or by asking it to \"re-evaluate its previous assessment.\" Consistent outcomes suggest higher stability. Self-Critique: Ask the LLM to critique its own initial verification and explanation, identifying any potential weaknesses or ambiguities. A strong, unconvincing self-critique could imply higher confidence in the original assertion. Utility: Can reveal the robustness of the LLM's conclusion. Current Thinking: Our primary focus will be on achieving accurate verification and high-quality explanations. For confidence scoring, we will likely start by combining Direct Prompting for Qualitative Score & Rationale (1) with Evidence-Based Heuristics (3). The numerical score will be presented as an estimate , with the textual explanation and the evidence characteristics providing richer context for the system's confidence.","title":"4. Approaches to Confidence Score Generation/Estimation"},{"location":"m4/#5-output-of-module-4","text":"The final output for a given claim will include: verification_status : (e.g., SUPPORTED, REFUTED, INSUFFICIENT_EVIDENCE, NUANCED). explanation_text : Detailed reasoning with references to evidence. confidence_score : The estimated confidence (e.g., a numerical score or qualitative label like \"High\"). confidence_rationale : Factors influencing the confidence score. cited_evidence_references : Links or identifiers to the specific evidence snippets crucial for the decision.","title":"5. Output of Module 4"},{"location":"dir1/m1/","text":"Module 1: Atomic Claim Extraction The primary objective of Module 1: Atomic Claim Extraction is to meticulously parse complex paragraphs and distill them into a series of discrete, atomic factual statements. Each extracted claim is intended to be contextually independent, allowing for focused verification in subsequent modules. Leveraging the powerful semantic understanding capabilities of advanced Large Language Models (LLMs), many traditional challenges in claim extraction are significantly mitigated. We anticipate minimal issues concerning co-reference resolution (i.e., correctly identifying who \"he\" or \"it\" refers to), ensuring the extracted claims are genuinely independent of their immediate textual surroundings, or judging whether a statement constitutes a verifiable factual assertion worthy of fact-checking. Despite these advantages, the iterative nature of claim extraction presents unique questions. The following outlines several key challenge explored and our current approach: Challenge 1: Determining Extraction Stoppage A critical question arises when the system continuously extracts claims from a paragraph without immediately identifying any factual errors: When should the claim extraction process for a given paragraph cease? Several strategies have been considered: Approach 1 (Attempted and Deemed Suboptimal): Relying on LLM Self-Termination Description: This approach involved allowing the LLM to determine when it had exhaustively extracted all perceivable claims from the paragraph. Outcome: Experimental results indicated that this method is unreliable. LLMs often concluded the extraction process prematurely, failing to identify all statements that would be valuable candidates for fact-checking. Approach 2 (Current Implemented Solution): Utilizing External Logic / Heuristic Algorithm Description: To ensure a more consistent and controllable extraction process, we employ an external logic to decide when to stop querying the LLM for more claims from the current paragraph. Current Heuristic: We are currently utilizing a straightforward threshold-based heuristic. The system stops extracting further claims when the total number of statements already extracted for the paragraph exceeds a value equivalent to (Total Number of Sentences in the Paragraph) / 2.5 . Performance: This empirical threshold provides an acceptable balance between extraction thoroughness and processing efficiency in our current testing scenarios, particularly as an initial greedy strategy when no errors are immediately apparent. Further refinement and exploration of more dynamic heuristics are ongoing.","title":"Module1"},{"location":"dir1/m1/#module-1-atomic-claim-extraction","text":"The primary objective of Module 1: Atomic Claim Extraction is to meticulously parse complex paragraphs and distill them into a series of discrete, atomic factual statements. Each extracted claim is intended to be contextually independent, allowing for focused verification in subsequent modules. Leveraging the powerful semantic understanding capabilities of advanced Large Language Models (LLMs), many traditional challenges in claim extraction are significantly mitigated. We anticipate minimal issues concerning co-reference resolution (i.e., correctly identifying who \"he\" or \"it\" refers to), ensuring the extracted claims are genuinely independent of their immediate textual surroundings, or judging whether a statement constitutes a verifiable factual assertion worthy of fact-checking. Despite these advantages, the iterative nature of claim extraction presents unique questions. The following outlines several key challenge explored and our current approach:","title":"Module 1: Atomic Claim Extraction"},{"location":"dir1/m1/#challenge-1-determining-extraction-stoppage","text":"A critical question arises when the system continuously extracts claims from a paragraph without immediately identifying any factual errors: When should the claim extraction process for a given paragraph cease? Several strategies have been considered: Approach 1 (Attempted and Deemed Suboptimal): Relying on LLM Self-Termination Description: This approach involved allowing the LLM to determine when it had exhaustively extracted all perceivable claims from the paragraph. Outcome: Experimental results indicated that this method is unreliable. LLMs often concluded the extraction process prematurely, failing to identify all statements that would be valuable candidates for fact-checking. Approach 2 (Current Implemented Solution): Utilizing External Logic / Heuristic Algorithm Description: To ensure a more consistent and controllable extraction process, we employ an external logic to decide when to stop querying the LLM for more claims from the current paragraph. Current Heuristic: We are currently utilizing a straightforward threshold-based heuristic. The system stops extracting further claims when the total number of statements already extracted for the paragraph exceeds a value equivalent to (Total Number of Sentences in the Paragraph) / 2.5 . Performance: This empirical threshold provides an acceptable balance between extraction thoroughness and processing efficiency in our current testing scenarios, particularly as an initial greedy strategy when no errors are immediately apparent. Further refinement and exploration of more dynamic heuristics are ongoing.","title":"Challenge 1: Determining Extraction Stoppage"}]}